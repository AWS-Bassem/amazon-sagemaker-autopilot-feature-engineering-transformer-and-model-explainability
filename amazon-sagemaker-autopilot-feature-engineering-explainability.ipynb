{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os, sys\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-automl-shap\"\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Role when working on a notebook instance\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "sm_rt = boto3.Session().client(\"runtime.sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the direct marketing dataset.\n",
    "\n",
    "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip --no-check-certificate\n",
    "!unzip -o bank-additional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip --no-check-certificate\n",
    "!unzip -o bank-additional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n",
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./bank-additional/bank-additional-full.csv\", sep=\";\")\n",
    "pd.set_option(\"display.max_columns\", 500)  # Make sure we can see all of the columns\n",
    "pd.set_option(\"display.max_rows\", 50)  # Keep the output on one page\n",
    "data[:10]  # Show the first 10 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, _ = np.split(\n",
    "    data.sample(frac=1, random_state=123), [int(0.95 * len(data)), int(len(data))]\n",
    ")\n",
    "\n",
    "# Save to CSV files\n",
    "train_data.to_csv(\n",
    "    \"automl-train.csv\", index=False, header=True, sep=\",\"\n",
    ")  # Need to keep column names\n",
    "test_data.to_csv(\"automl-test.csv\", index=False, header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.upload_data(path=\"automl-train.csv\", key_prefix=prefix + \"/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(\"y\", axis=1).to_csv(\n",
    "    \"automl-validation.csv\", index=False, header=True, sep=\",\"\n",
    ")\n",
    "sess.upload_data(path=\"automl-validation.csv\", key_prefix=prefix + \"/validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = {\n",
    "    \"CompletionCriteria\": {\n",
    "        \"MaxRuntimePerTrainingJobInSeconds\": 300,\n",
    "        \"MaxCandidates\": 50,\n",
    "        \"MaxAutoMLJobRuntimeInSeconds\": 3600,\n",
    "    }\n",
    "}\n",
    "\n",
    "input_data_config = [\n",
    "    {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": f\"s3://{bucket}/{prefix}/input\",\n",
    "            }\n",
    "        },\n",
    "        \"TargetAttributeName\": \"y\",\n",
    "    }\n",
    "]\n",
    "\n",
    "output_data_config = {\"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\"}\n",
    "\n",
    "problem_type = \"BinaryClassification\"\n",
    "\n",
    "job_objective = {\"MetricName\": \"F1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "timestamp_suffix = strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "auto_ml_job_name = \"automl-shap-\" + timestamp_suffix\n",
    "print(\"AutoMLJobName: \" + auto_ml_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.create_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    AutoMLJobConfig=job_config,\n",
    "    AutoMLJobObjective=job_objective,\n",
    "    ProblemType=problem_type,\n",
    "    RoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "job_run_status = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)[\n",
    "    \"AutoMLJobStatus\"\n",
    "]\n",
    "print(job_run_status)\n",
    "\n",
    "while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "    print(\n",
    "        describe_response[\"AutoMLJobStatus\"]\n",
    "        + \" - \"\n",
    "        + describe_response[\"AutoMLJobSecondaryStatus\"]\n",
    "    )\n",
    "    sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-generated Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "job_candidate_notebook = job[\"AutoMLJobArtifacts\"][\n",
    "    \"CandidateDefinitionNotebookLocation\"\n",
    "]\n",
    "job_data_notebook = job[\"AutoMLJobArtifacts\"][\"DataExplorationNotebookLocation\"]\n",
    "\n",
    "print(job_candidate_notebook)\n",
    "print(job_data_notebook)\n",
    "\n",
    "\n",
    "def download_gen_notebook(path):\n",
    "    bucket, key = path.split(\"/\", 2)[-1].split(\"/\", 1)\n",
    "    boto3.client(\"s3\").download_file(\n",
    "        Bucket=bucket, Key=key, Filename=key.split(\"/\")[-1]\n",
    "    )\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "download_gen_notebook(job_candidate_notebook)\n",
    "download_gen_notebook(job_data_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the Experiment Candidates by AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    experiment_name=auto_ml_job_name + \"-aws-auto-ml-job\",\n",
    ")\n",
    "analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the Model Tuning by AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = sm.list_candidates_for_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name, SortBy=\"FinalObjectiveMetricValue\"\n",
    ")[\"Candidates\"]\n",
    "\n",
    "index = 1\n",
    "for candidate in candidates:\n",
    "    print(\n",
    "        str(index)\n",
    "        + \"  \"\n",
    "        + candidate[\"CandidateName\"]\n",
    "        + \"  \"\n",
    "        + str(candidate[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"])\n",
    "    )\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.list_candidates_for_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "\n",
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)[\n",
    "    \"BestCandidate\"\n",
    "]\n",
    "best_candidate_name = best_candidate[\"CandidateName\"]\n",
    "\n",
    "print(\"Candidate name: \" + best_candidate_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate[\"InferenceContainers\"]\n",
    "\n",
    "for container in best_candidate[\"InferenceContainers\"]:\n",
    "    print(container[\"Image\"])\n",
    "    print(container[\"ModelDataUrl\"])\n",
    "    print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate[\"InferenceContainers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_chain(best_candidate, names):\n",
    "    assert len(best_candidate[\"InferenceContainers\"]) == len(names)\n",
    "    model_chain_list = []\n",
    "    container = best_candidate[\"InferenceContainers\"][0]\n",
    "\n",
    "    model = sagemaker.model.Model(\n",
    "        model_data=container[\"ModelDataUrl\"],\n",
    "        image=container[\"Image\"],\n",
    "        env=container[\"Environment\"],\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        name=\"feature_engineering\",\n",
    "    )\n",
    "\n",
    "    return [model]\n",
    "\n",
    "\n",
    "res = model_chain(best_candidate, [\"feature_engineering\", \"model\", \"label_transform\"])\n",
    "\n",
    "ppl = sagemaker.pipeline.PipelineModel(\n",
    "    res, role=role, name=\"fe-chain-diy\", sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in res:\n",
    "    print(r.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer = res[0]\n",
    "data_transformer.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download the generated artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {data_transformer.model_data} feature_engineering_data_transformer.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf feature_engineering_data_transformer.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "data_transformer = load(filename=\"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer.feature_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer.feature_transformer[\"column_transformer\"].transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id = data_transformer.feature_transformer[\"column_transformer\"].transformers_[\n",
    "    1\n",
    "][2]\n",
    "new_cat_col = (\n",
    "    data_transformer.feature_transformer[\"column_transformer\"]\n",
    "    .transformers_[1][1][\"thresholdonehotencoder\"]\n",
    "    .get_feature_names(data.columns[category_id])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_feature_names(columnTransformer):\n",
    "\n",
    "    output_features = []\n",
    "\n",
    "    for name, pipe, features in columnTransformer.transformers_:\n",
    "        print(name, features)\n",
    "        if name != \"remainder\":\n",
    "            for i in pipe:\n",
    "                trans_features = []\n",
    "                if hasattr(i, \"categories_\"):\n",
    "                    trans_features.extend(i.get_feature_names(data.columns[features]))\n",
    "                else:\n",
    "                    trans_features = data.columns[features]\n",
    "            output_features.extend(trans_features)\n",
    "\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = get_transformer_feature_names(\n",
    "    data_transformer.feature_transformer[\"column_transformer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering PipelineModel Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data capture configuration\n",
    "s3_capture_path = f\"s3://{bucket}/\" + \"capture\" + \"/\"\n",
    "print(s3_capture_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_config = sagemaker.model_monitor.DataCaptureConfig(\n",
    "    True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=s3_capture_path,\n",
    "    capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "    csv_content_types=[\"text/csv\"],\n",
    "    json_content_types=[\"application/json\"],\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    endpoint_name=ppl.name,\n",
    "    update_endpoint=False,\n",
    "    wait=True,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enigneering Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV\n",
    "\n",
    "\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint=\"fe-chain-diy\",\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    content_type=CONTENT_TYPE_CSV,\n",
    "    accept=CONTENT_TYPE_CSV,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_2_transform = test_data.drop(\"y\", axis=1)\n",
    "test_data_transformed = predictor.predict(\n",
    "    test_data_2_transform.to_csv(sep=\",\", header=None, index=False)\n",
    ").decode(\"utf-8\")\n",
    "test_data_transformed_df = pd.read_csv(StringIO(test_data_transformed), header=None)\n",
    "test_data_transformed_df.columns = new_col\n",
    "test_data_transformed_df[\"y\"] = test_data[\"y\"].values\n",
    "test_data_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_2_transform = train_data.drop(\"y\", axis=1)\n",
    "train_data_transformed = predictor.predict(\n",
    "    train_data_2_transform.to_csv(sep=\",\", header=None, index=False)\n",
    ").decode(\"utf-8\")\n",
    "train_data_transformed_df = pd.read_csv(StringIO(train_data_transformed), header=None)\n",
    "train_data_transformed_df.columns = new_col\n",
    "train_data_transformed_df[\"y\"] = train_data[\"y\"].values\n",
    "train_data_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prediction_df.shape[0] == test_data.shape[0]\n",
    "sum(prediction_df.values[:,0] == test_data['y'].values) / len(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed_df.to_csv(\n",
    "    \"automl-transformed-train-2nd.csv\", index=False, header=False, sep=\",\"\n",
    ")\n",
    "test_data_transformed_df.to_csv(\n",
    "    \"automl-transformed-test-2nd.csv\", index=False, header=False, sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Feature Engineering Transformation Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=\"fe-chain-diy\")\n",
    "sm.delete_endpoint_config(EndpointConfigName=\"fe-chain-diy\")\n",
    "sm.delete_model(ModelName=\"fe-chain-diy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install shap\n",
    "!python -m pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_tunning_name = \"-\".join(best_candidate_name.split(\"-\")[:4])\n",
    "tuner = sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    hyperparameter_tuning_job_name=hyper_tunning_name\n",
    ")\n",
    "\n",
    "full_df = tuner.dataframe().sort_values(\"FinalObjectiveValue\", ascending=False)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_job_summary = pd.DataFrame(tuner.training_job_summaries())\n",
    "tune_job_summary[\"MetricValue\"] = tune_job_summary.apply(\n",
    "    lambda x: x[\"FinalHyperParameterTuningJobObjectiveMetric\"][\"Value\"], axis=1\n",
    ")\n",
    "tune_job_summary.sort_values(\n",
    "    by=\"MetricValue\", ascending=False, na_position=\"first\", inplace=True\n",
    ")\n",
    "best_hyper = tune_job_summary.iloc[[0]][\"TunedHyperParameters\"].values[0]\n",
    "model_type = (\n",
    "    tune_job_summary.iloc[[0]][\"TrainingJobDefinitionName\"].values[0].split(\"-\")[-1]\n",
    ")\n",
    "best_hyper, model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"xgb\":\n",
    "    container = get_image_uri(region, \"xgboost\", repo_version=\"0.90-2\")\n",
    "\n",
    "base_job_name = \"smdebug-xgboost-prediction\"\n",
    "bucket_path = f\"s3://{bucket}\"\n",
    "save_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_estimator = Estimator(\n",
    "    role=role,\n",
    "    base_job_name=base_job_name,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.m5.4xlarge\",\n",
    "    image_name=container,\n",
    "    hyperparameters=best_hyper,\n",
    "    train_max_run=1800,\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        s3_output_path=bucket_path,  # Required\n",
    "        collection_configs=[\n",
    "            CollectionConfig(\n",
    "                name=\"metrics\", parameters={\"save_interval\": str(save_interval)}\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"feature_importance\",\n",
    "                parameters={\"save_interval\": str(save_interval)},\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"full_shap\", parameters={\"save_interval\": str(save_interval)}\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"average_shap\", parameters={\"save_interval\": str(save_interval)}\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\",\n",
    "                \"num_steps\": str(save_interval * 2),\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "sess.upload_data(\n",
    "    path=\"automl-transformed-train-2nd.csv\", key_prefix=prefix + \"/transformedtrain-2nd\"\n",
    ")\n",
    "sess.upload_data(\n",
    "    path=\"automl-transformed-test-2nd.csv\", key_prefix=prefix + \"/transformedtest-2nd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = s3_input(\n",
    "    \"s3://{}/{}/{}\".format(\n",
    "        bucket, prefix, \"transformedtrain-2nd/automl-transformed-train-2nd.csv\"\n",
    "    ),\n",
    "    content_type=\"csv\",\n",
    ")\n",
    "validation_input = s3_input(\n",
    "    \"s3://{}/{}/{}\".format(\n",
    "        bucket, prefix, \"transformedtest-2nd/automl-transformed-test-2nd.csv\"\n",
    "    ),\n",
    "    content_type=\"csv\",\n",
    ")\n",
    "xgboost_estimator.fit(\n",
    "    {\"train\": train_input, \"validation\": validation_input},\n",
    "    # This is a fire and forget event. By setting wait=False, you submit the job to run in the background.\n",
    "    # Amazon SageMaker starts one training job and release control to next cells in the notebook.\n",
    "    # Follow this notebook to see status of the training job.\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(36):\n",
    "    job_name = xgboost_estimator.latest_training_job.name\n",
    "    client = xgboost_estimator.sagemaker_session.sagemaker_client\n",
    "    description = client.describe_training_job(TrainingJobName=job_name)\n",
    "    training_job_status = description[\"TrainingJobStatus\"]\n",
    "    rule_job_summary = xgboost_estimator.latest_training_job.rule_job_summary()\n",
    "    rule_evaluation_status = rule_job_summary[0][\"RuleEvaluationStatus\"]\n",
    "    print(\n",
    "        \"Training job status: {}, Rule Evaluation Status: {}\".format(\n",
    "            training_job_status, rule_evaluation_status\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if training_job_status in [\"Completed\", \"Failed\"]:\n",
    "        break\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_estimator.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = xgboost_estimator.latest_job_debugger_artifacts_path()\n",
    "trial = create_trial(s3_output_path)\n",
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "MAX_PLOTS = 35\n",
    "\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    \"\"\"\n",
    "    For the given tensor name, walks though all the iterations\n",
    "    for which you have data and fetches the values.\n",
    "    Returns the set of steps and the values.\n",
    "    \"\"\"\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals\n",
    "\n",
    "\n",
    "def plot_collection(trial, collection_name, regex=\".*\", figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Takes a `trial` and a collection name, and \n",
    "    plots all tensors that match the given regex.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    tensors = sorted(trial.collection(collection_name).tensor_names)\n",
    "    matched_tensors = [t for t in tensors if re.match(regex, t)]\n",
    "    for tensor_name in islice(matched_tensors, MAX_PLOTS):\n",
    "        steps, data = get_data(trial, tensor_name)\n",
    "        ax.plot(steps, data, label=tensor_name)\n",
    "\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial, \"metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(trial, importance_type=\"weight\"):\n",
    "    SUPPORTED_IMPORTANCE_TYPES = [\n",
    "        \"weight\",\n",
    "        \"gain\",\n",
    "        \"cover\",\n",
    "        \"total_gain\",\n",
    "        \"total_cover\",\n",
    "    ]\n",
    "    if importance_type not in SUPPORTED_IMPORTANCE_TYPES:\n",
    "        raise ValueError(\n",
    "            f\"{importance_type} is not one of the supported importance types.\"\n",
    "        )\n",
    "    plot_collection(\n",
    "        trial, \"feature_importance\", regex=f\"feature_importance/{importance_type}/.*\"\n",
    "    )\n",
    "\n",
    "\n",
    "plot_feature_importance(trial, importance_type=\"cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial, \"average_shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap_values = trial.tensor(\"full_shap/f0\").value(trial.last_complete_step)\n",
    "shap_no_base = shap_values[:, :-1]\n",
    "shap_base_value = shap_values[0, -1]\n",
    "shap.summary_plot(shap_no_base, plot_type=\"bar\", feature_names=new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_no_base, train_data_transformed_df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4000\n",
    "shap.force_plot(\n",
    "    shap_base_value,\n",
    "    shap_no_base[idx, :],\n",
    "    train_data_transformed_df.iloc[idx, :-1],\n",
    "    link=\"logit\",\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(\n",
    "    shap_base_value, shap_no_base[idx, :], train_data_transformed_df.iloc[idx, :-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_ROWS = shap_no_base.shape[0]\n",
    "N_SAMPLES = min(100, N_ROWS)\n",
    "sampled_indices = np.random.randint(N_ROWS, size=N_SAMPLES)\n",
    "\n",
    "shap.force_plot(\n",
    "    shap_base_value,\n",
    "    shap_no_base[sampled_indices, :],\n",
    "    train_data_transformed_df.iloc[sampled_indices, :-1],\n",
    "    link=\"logit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top outliers\n",
    "from scipy import stats\n",
    "\n",
    "N_OUTLIERS = 3  # number of outliers on each side of the tail\n",
    "\n",
    "shap_sum = np.sum(shap_no_base, axis=1)\n",
    "z_scores = stats.zscore(shap_sum)\n",
    "outlier_indices = (np.argpartition(z_scores, -N_OUTLIERS)[-N_OUTLIERS:]).tolist()\n",
    "outlier_indices += (np.argpartition(z_scores, N_OUTLIERS)[:N_OUTLIERS]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fig_index, outlier_index in enumerate(outlier_indices, start=1):\n",
    "    shap.force_plot(\n",
    "        shap_base_value,\n",
    "        shap_no_base[outlier_index, :],\n",
    "        train_data_transformed_df.iloc[outlier_index, :-1],\n",
    "        matplotlib=True,\n",
    "        link=\"logit\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmax",
   "language": "python",
   "name": "mlmax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}